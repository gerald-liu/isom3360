{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "train = pd.read_csv('train_titanic.csv',index_col = 'PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define attributes and target variable\n",
    "features = ['Pclass','Age','SibSp','Parch','Fare','Sex_male','Embarked_Q','Embarked_S']\n",
    "target = ['Survived']\n",
    "X = train [features]\n",
    "y = train [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize attributes\n",
    "zscore_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_transform = pd.DataFrame(zscore_scaler.transform(X), columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  \\\n",
       "PassengerId                                                              \n",
       "1                 3  22.0      1      0   7.2500         1           0   \n",
       "2                 1  38.0      1      0  71.2833         0           0   \n",
       "3                 3  26.0      0      0   7.9250         0           0   \n",
       "4                 1  35.0      1      0  53.1000         0           0   \n",
       "5                 3  35.0      0      0   8.0500         1           0   \n",
       "\n",
       "             Embarked_S  \n",
       "PassengerId              \n",
       "1                     1  \n",
       "2                     0  \n",
       "3                     1  \n",
       "4                     1  \n",
       "5                     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.592481</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>0.619306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.638789</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>-1.614710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.284663</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>0.619306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.407926</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>0.619306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.407926</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.307562</td>\n",
       "      <td>0.619306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Age     SibSp     Parch      Fare  Sex_male  Embarked_Q  \\\n",
       "0  0.827377 -0.592481  0.432793 -0.473674 -0.502445  0.737695   -0.307562   \n",
       "1 -1.566107  0.638789  0.432793 -0.473674  0.786845 -1.355574   -0.307562   \n",
       "2  0.827377 -0.284663 -0.474545 -0.473674 -0.488854 -1.355574   -0.307562   \n",
       "3 -1.566107  0.407926  0.432793 -0.473674  0.420730 -1.355574   -0.307562   \n",
       "4  0.827377  0.407926 -0.474545 -0.473674 -0.486337  0.737695   -0.307562   \n",
       "\n",
       "   Embarked_S  \n",
       "0    0.619306  \n",
       "1   -1.614710  \n",
       "2    0.619306  \n",
       "3    0.619306  \n",
       "4    0.619306  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transform.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Logistic Regression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model to be logistic regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# penalty='l1' means L1 regularization (recall LASSO regression); default is penality='L2' (L2 regularization). C=1.0 is inverse of regularization strength; must be a positive float.\n",
    "# 'saga' is the algorithm to use in the optimization problem (finding the optimal coefficient values)\n",
    "lr = LogisticRegression(penalty='l1', C=1.0, random_state=0, solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cross validation and other evaluation tool \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change target column to array\n",
    "yact = y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cv = cross_val_score(lr, X_transform, yact, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7946192259675405"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy from cross validation\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(lr, X_transform, yact, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[469  80]\n",
      " [103 239]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "print(confusion_matrix(y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84       549\n",
      "           1       0.75      0.70      0.72       342\n",
      "\n",
      "    accuracy                           0.79       891\n",
      "   macro avg       0.78      0.78      0.78       891\n",
      "weighted avg       0.79      0.79      0.79       891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yact, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probablities for each prediction\n",
    "proba_y = cross_val_predict(lr, X_transform, yact, cv=10, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90383872, 0.09616128],\n",
       "       [0.07386228, 0.92613772],\n",
       "       [0.38423029, 0.61576971],\n",
       "       ...,\n",
       "       [0.50538202, 0.49461798],\n",
       "       [0.41695825, 0.58304175],\n",
       "       [0.88578243, 0.11421757]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90383872, 0.07386228, 0.38423029, 0.09674271, 0.9263234 ,\n",
       "       0.87998415, 0.72065533, 0.88404409, 0.47767433, 0.09465835,\n",
       "       0.2759629 , 0.18518448, 0.87460769, 0.97647098, 0.28033079,\n",
       "       0.38630605, 0.86975736, 0.76866383, 0.4793171 , 0.33366429,\n",
       "       0.79841002, 0.79733545, 0.22548364, 0.49186857, 0.40540937,\n",
       "       0.73811437, 0.87634869, 0.51183712, 0.34158843, 0.91082117,\n",
       "       0.52280178, 0.04549201, 0.34166134, 0.93295403, 0.42580493,\n",
       "       0.66679593, 0.87634755, 0.87885256, 0.40716438, 0.24961236,\n",
       "       0.57251918, 0.20463991, 0.8761662 , 0.08564855, 0.25417042,\n",
       "       0.91078974, 0.89955949, 0.34166134, 0.9135275 , 0.35596383,\n",
       "       0.91673026, 0.87891932, 0.10809036, 0.21559447, 0.76185023,\n",
       "       0.50855264, 0.14373489, 0.87115231, 0.13151654, 0.9493578 ,\n",
       "       0.83968167, 0.05905482, 0.67537528, 0.90621341, 0.422296  ,\n",
       "       0.91144604, 0.18688728, 0.87020536, 0.59791633, 0.93162545,\n",
       "       0.78540292, 0.6171449 , 0.66979926, 0.88211197, 0.90822426,\n",
       "       0.89470566, 0.91082117, 0.91078974, 0.59331263, 0.4192326 ,\n",
       "       0.88272642, 0.90823032, 0.34350483, 0.48459649, 0.13453399,\n",
       "       0.67413534, 0.92153869, 0.91078974, 0.09402649, 0.89084979,\n",
       "       0.9085331 , 0.87466156, 0.69589613, 0.93792646, 0.96665711,\n",
       "       0.91056841, 0.73849307, 0.33333291, 0.22479656, 0.84190437,\n",
       "       0.40472018, 0.91059757, 0.43568889, 0.91959556, 0.96145043,\n",
       "       0.90554226, 0.34642127, 0.91062041, 0.93193951, 0.40901243,\n",
       "       0.66383008, 0.24921872, 0.88555594, 0.41148337, 0.20893738,\n",
       "       0.88192332, 0.96932343, 0.81844323, 0.25250866, 0.49918064,\n",
       "       0.80476835, 0.91056841, 0.74486542, 0.21170152, 0.71458105,\n",
       "       0.81180375, 0.88064896, 0.89278764, 0.36986733, 0.9462698 ,\n",
       "       0.86994827, 0.87837749, 0.64424255, 0.24012477, 0.74348291,\n",
       "       0.61063347, 0.05672226, 0.65453568, 0.86170522, 0.32282439,\n",
       "       0.31272457, 0.35448309, 0.44294071, 0.83472622, 0.69383117,\n",
       "       0.76120298, 0.90246906, 0.40220658, 0.82302323, 0.84160019,\n",
       "       0.87996795, 0.07087847, 0.96232551, 0.94178874, 0.91070781,\n",
       "       0.57679223, 0.24306517, 0.91143804, 0.91045248, 0.99193534,\n",
       "       0.94574267, 0.25851139, 0.89929033, 0.86604837, 0.92698181,\n",
       "       0.83845335, 0.07289749, 0.66318063, 0.51352021, 0.90797557,\n",
       "       0.77800814, 0.9197325 , 0.27058014, 0.88810239, 0.64819071,\n",
       "       0.91734153, 0.971365  , 0.08181464, 0.77670174, 0.93456756,\n",
       "       0.92395561, 0.6856112 , 0.95967705, 0.6963656 , 0.23483988,\n",
       "       0.50701304, 0.41387793, 0.66110614, 0.943527  , 0.93404337,\n",
       "       0.20874218, 0.69465108, 0.40398027, 0.64882382, 0.07212589,\n",
       "       0.08855971, 0.87821728, 0.95212774, 0.34352447, 0.14373163,\n",
       "       0.91213569, 0.99501802, 0.92922473, 0.92856874, 0.87605266,\n",
       "       0.20495192, 0.94427112, 0.85811414, 0.20542771, 0.49829405,\n",
       "       0.89918915, 0.2028307 , 0.89195157, 0.77670174, 0.91036062,\n",
       "       0.0521335 , 0.37836629, 0.88462628, 0.04053558, 0.77717037,\n",
       "       0.86742385, 0.75597725, 0.96192395, 0.91738342, 0.65646688,\n",
       "       0.89173247, 0.69522434, 0.88624367, 0.68640278, 0.67359386,\n",
       "       0.09442712, 0.91532512, 0.91414976, 0.55371466, 0.7345157 ,\n",
       "       0.4032604 , 0.89237843, 0.09980653, 0.69522434, 0.79626801,\n",
       "       0.37519067, 0.37991146, 0.77041442, 0.8919646 , 0.87724739,\n",
       "       0.68777944, 0.36042388, 0.17234808, 0.68093727, 0.92423033,\n",
       "       0.91743635, 0.49105935, 0.76034872, 0.93257472, 0.5245871 ,\n",
       "       0.340052  , 0.0371064 , 0.05735667, 0.02874781, 0.33480733,\n",
       "       0.88184693, 0.94969707, 0.75652198, 0.58552605, 0.33809976,\n",
       "       0.79249529, 0.96125588, 0.92880347, 0.15677098, 0.06540311,\n",
       "       0.48039034, 0.9033027 , 0.26368834, 0.46573718, 0.30492965,\n",
       "       0.2361347 , 0.54014673, 0.75237435, 0.93271515, 0.56216601,\n",
       "       0.96618912, 0.89989908, 0.85043501, 0.88018348, 0.48168779,\n",
       "       0.88041143, 0.91812906, 0.87738241, 0.84680268, 0.27596183,\n",
       "       0.04970294, 0.03713349, 0.72080853, 0.34550247, 0.88533336,\n",
       "       0.3859998 , 0.83706144, 0.03008671, 0.48052008, 0.07235358,\n",
       "       0.33809976, 0.93823097, 0.86552824, 0.13774737, 0.9055541 ,\n",
       "       0.30314089, 0.037964  , 0.03389574, 0.74498807, 0.04050866,\n",
       "       0.03166581, 0.04717472, 0.21022286, 0.89989518, 0.8812689 ,\n",
       "       0.36313795, 0.18985813, 0.88298008, 0.06023419, 0.0793958 ,\n",
       "       0.87745461, 0.89641669, 0.13911213, 0.18560476, 0.99454547,\n",
       "       0.04659666, 0.96931507, 0.20679112, 0.50745862, 0.02540364,\n",
       "       0.50969447, 0.62832749, 0.54021476, 0.92098818, 0.07560225,\n",
       "       0.90556781, 0.55509097, 0.05587326, 0.94492621, 0.62216706,\n",
       "       0.60956324, 0.11653984, 0.73753257, 0.7148565 , 0.79206763,\n",
       "       0.14175938, 0.2328564 , 0.48342483, 0.83910423, 0.93864163,\n",
       "       0.88127106, 0.47935257, 0.84914092, 0.919458  , 0.86672295,\n",
       "       0.89974487, 0.05581948, 0.21954588, 0.32193008, 0.32193008,\n",
       "       0.96260244, 0.73087199, 0.45427948, 0.92283559, 0.90101387,\n",
       "       0.90732403, 0.16909126, 0.2986941 , 0.32200396, 0.03481923,\n",
       "       0.43229295, 0.89214802, 0.86296979, 0.28682091, 0.37903218,\n",
       "       0.05626287, 0.33118172, 0.32005536, 0.80730529, 0.86305496,\n",
       "       0.04661573, 0.13412329, 0.91368335, 0.11193888, 0.90616535,\n",
       "       0.64326151, 0.93814223, 0.23177977, 0.87217802, 0.08224536,\n",
       "       0.64841439, 0.87221464, 0.94358054, 0.04035994, 0.37959658,\n",
       "       0.87659822, 0.41455884, 0.86196783, 0.72195553, 0.17994363,\n",
       "       0.93332351, 0.89279519, 0.39151034, 0.92326545, 0.31294546,\n",
       "       0.84215299, 0.95764514, 0.62737443, 0.87222071, 0.63660468,\n",
       "       0.90616535, 0.87243215, 0.06925169, 0.77714664, 0.94472107,\n",
       "       0.40189042, 0.28041214, 0.1467474 , 0.77330556, 0.25546916,\n",
       "       0.85931446, 0.828222  , 0.90376998, 0.47620179, 0.89615277,\n",
       "       0.9063091 , 0.2238866 , 0.12887097, 0.87217195, 0.91365753,\n",
       "       0.51811198, 0.47249278, 0.33530597, 0.85355525, 0.76537355,\n",
       "       0.05064057, 0.48994212, 0.30141839, 0.81931645, 0.78134813,\n",
       "       0.38288653, 0.8671855 , 0.9159815 , 0.17744602, 0.90846522,\n",
       "       0.3059274 , 0.11530942, 0.57749891, 0.26486157, 0.73251684,\n",
       "       0.86909637, 0.9272105 , 0.42338219, 0.66267233, 0.906131  ,\n",
       "       0.87005419, 0.82189671, 0.09066814, 0.34008174, 0.86444318,\n",
       "       0.70308011, 0.9215813 , 0.68734473, 0.8754868 , 0.90848046,\n",
       "       0.93238763, 0.78077153, 0.76422873, 0.86445177, 0.23367732,\n",
       "       0.90867522, 0.93208909, 0.29968836, 0.10918329, 0.32930507,\n",
       "       0.51727204, 0.84573436, 0.93052784, 0.88022735, 0.19788297,\n",
       "       0.96073181, 0.78077153, 0.95656244, 0.71066895, 0.43228063,\n",
       "       0.65050932, 0.09892527, 0.70650653, 0.90945762, 0.86850528,\n",
       "       0.92985057, 0.8761134 , 0.75494799, 0.79095269, 0.87585869,\n",
       "       0.87097916, 0.14235242, 0.90674792, 0.07081289, 0.8881794 ,\n",
       "       0.85753807, 0.23364072, 0.30025668, 0.46957424, 0.03665686,\n",
       "       0.35445124, 0.23726098, 0.53586498, 0.89899492, 0.88163894,\n",
       "       0.86119545, 0.90848046, 0.59671185, 0.14923785, 0.8882667 ,\n",
       "       0.69015901, 0.21569662, 0.85871138, 0.28216439, 0.91575205,\n",
       "       0.06066351, 0.88011142, 0.87334282, 0.08713721, 0.87334146,\n",
       "       0.90692251, 0.34008174, 0.3944874 , 0.93466393, 0.84885548,\n",
       "       0.10193774, 0.87334146, 0.86534612, 0.33349493, 0.40278134,\n",
       "       0.09415474, 0.67792664, 0.0462249 , 0.9068965 , 0.04290639,\n",
       "       0.09364471, 0.5327471 , 0.55207177, 0.83604086, 0.68146588,\n",
       "       0.82140893, 0.17380999, 0.71074274, 0.93984376, 0.67092135,\n",
       "       0.31587349, 0.75164886, 0.86456366, 0.83727257, 0.33160885,\n",
       "       0.80948458, 0.13819531, 0.34013633, 0.13091721, 0.53663897,\n",
       "       0.86458708, 0.93591845, 0.76457518, 0.90720061, 0.40086763,\n",
       "       0.93531151, 0.86573334, 0.42984457, 0.8741128 , 0.91451099,\n",
       "       0.92478951, 0.27001359, 0.60701452, 0.30409953, 0.85151794,\n",
       "       0.86378274, 0.21943245, 0.13264606, 0.39287428, 0.91449701,\n",
       "       0.21577031, 0.08981252, 0.89647478, 0.51397453, 0.87370001,\n",
       "       0.04776182, 0.87145604, 0.77962588, 0.87869853, 0.90720061,\n",
       "       0.92334205, 0.14525955, 0.95052485, 0.32341725, 0.86101338,\n",
       "       0.94564943, 0.18438159, 0.95485683, 0.87411397, 0.69985   ,\n",
       "       0.2699487 , 0.90723339, 0.53718604, 0.94462816, 0.51281267,\n",
       "       0.94444721, 0.90821462, 0.51848761, 0.13194655, 0.10326068,\n",
       "       0.61133597, 0.90741301, 0.36775877, 0.86458708, 0.92317654,\n",
       "       0.19743212, 0.94284697, 0.44047316, 0.13275982, 0.7516882 ,\n",
       "       0.89486714, 0.73128888, 0.86487547, 0.87454131, 0.87223857,\n",
       "       0.8010956 , 0.86797519, 0.05061138, 0.89438066, 0.86459202,\n",
       "       0.90345126, 0.95737589, 0.46316766, 0.56367745, 0.43627741,\n",
       "       0.17556403, 0.91895427, 0.83579646, 0.50590007, 0.93516011,\n",
       "       0.87359384, 0.03881068, 0.36700101, 0.91087827, 0.20978695,\n",
       "       0.69869965, 0.86896606, 0.69882942, 0.91160222, 0.3390003 ,\n",
       "       0.91159713, 0.12236583, 0.87806896, 0.29370762, 0.20424791,\n",
       "       0.8423623 , 0.91159713, 0.38793533, 0.72545973, 0.70927121,\n",
       "       0.83935345, 0.91334932, 0.70586805, 0.93043182, 0.90637077,\n",
       "       0.88142468, 0.74158591, 0.91159891, 0.94695246, 0.09700405,\n",
       "       0.32554234, 0.63353509, 0.94840646, 0.78613574, 0.77738481,\n",
       "       0.86419868, 0.89271224, 0.2943247 , 0.60652203, 0.4800707 ,\n",
       "       0.29369593, 0.4103906 , 0.87356615, 0.96594901, 0.943826  ,\n",
       "       0.7240473 , 0.95340993, 0.86891868, 0.86419823, 0.03869121,\n",
       "       0.63332277, 0.1341151 , 0.91087827, 0.85022068, 0.80401887,\n",
       "       0.89735336, 0.94898728, 0.29371126, 0.70412891, 0.944845  ,\n",
       "       0.04113198, 0.59390962, 0.21625884, 0.83934858, 0.92537193,\n",
       "       0.83609989, 0.28355885, 0.62648056, 0.049206  , 0.90474667,\n",
       "       0.0374935 , 0.54025985, 0.733771  , 0.9092208 , 0.89736178,\n",
       "       0.86897118, 0.05327015, 0.17131679, 0.86363984, 0.92197655,\n",
       "       0.09673742, 0.89533985, 0.79108116, 0.87111196, 0.56280516,\n",
       "       0.87230542, 0.35955504, 0.30210804, 0.78498004, 0.4369391 ,\n",
       "       0.05982956, 0.75976103, 0.76600389, 0.71771121, 0.71771121,\n",
       "       0.9023377 , 0.71495296, 0.36523079, 0.90664233, 0.90664233,\n",
       "       0.51719834, 0.63606241, 0.06411948, 0.91373057, 0.91055196,\n",
       "       0.87168605, 0.8991178 , 0.18699637, 0.49071823, 0.86600343,\n",
       "       0.11929968, 0.82228383, 0.9161832 , 0.88398597, 0.42108529,\n",
       "       0.63256691, 0.90130967, 0.67988775, 0.91900674, 0.07501469,\n",
       "       0.90620427, 0.93602357, 0.81552305, 0.13130593, 0.85536926,\n",
       "       0.17449697, 0.40733322, 0.30825526, 0.89145395, 0.91343065,\n",
       "       0.88752306, 0.9495991 , 0.3798932 , 0.86265521, 0.51302716,\n",
       "       0.86410249, 0.86044234, 0.21739667, 0.86044352, 0.1064735 ,\n",
       "       0.19412472, 0.05903026, 0.51087453, 0.94274844, 0.89127322,\n",
       "       0.89125807, 0.30868612, 0.91173339, 0.85375696, 0.54594464,\n",
       "       0.86044234, 0.66139015, 0.90076773, 0.4090153 , 0.89120911,\n",
       "       0.81943265, 0.13177498, 0.40429284, 0.8639406 , 0.50877481,\n",
       "       0.79108116, 0.24878919, 0.44797231, 0.73385198, 0.89921654,\n",
       "       0.91056151, 0.60565221, 0.2987498 , 0.82389316, 0.09608336,\n",
       "       0.89579049, 0.93051598, 0.80264365, 0.49564571, 0.90992422,\n",
       "       0.5398781 , 0.33763015, 0.78210798, 0.94070155, 0.92916667,\n",
       "       0.17999921, 0.89899412, 0.61254553, 0.39283159, 0.92002304,\n",
       "       0.87727554, 0.90145982, 0.50679464, 0.87715033, 0.13798373,\n",
       "       0.26603853, 0.63168472, 0.87723185, 0.88529828, 0.86568563,\n",
       "       0.09220908, 0.87767275, 0.90753834, 0.90856186, 0.44968039,\n",
       "       0.87388172, 0.6726968 , 0.0516417 , 0.89473766, 0.86139098,\n",
       "       0.93857373, 0.98998789, 0.89622034, 0.77208815, 0.06116499,\n",
       "       0.93264836, 0.97968074, 0.24414024, 0.04717982, 0.32319547,\n",
       "       0.31991327, 0.13336924, 0.70815015, 0.33873176, 0.87723185,\n",
       "       0.9619556 , 0.76370997, 0.12588941, 0.86721294, 0.73187185,\n",
       "       0.25604108, 0.16100153, 0.53325364, 0.9073614 , 0.84970377,\n",
       "       0.89578937, 0.16294696, 0.56737689, 0.94804481, 0.16388379,\n",
       "       0.21774064, 0.87357381, 0.86987417, 0.90755714, 0.124458  ,\n",
       "       0.16870808, 0.91703567, 0.32881283, 0.75977665, 0.89250612,\n",
       "       0.51456467, 0.7524836 , 0.04808908, 0.50538202, 0.41695825,\n",
       "       0.88578243])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities of being '0' for all instances at default threshold of 0.5\n",
    "proba_y_0 = proba_y[:,0]\n",
    "proba_y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower threshold for '0' to be 0.4\n",
    "proba_y_0_lt= [0 if i >= 0.4 else 1 for i in proba_y_0]\n",
    "proba_y_0_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[503  46]\n",
      " [124 218]]\n",
      "Accuracy: 0.8092031425364759\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\", '\\n',confusion_matrix(y, proba_y_0_lt))\n",
    "print(\"Accuracy:\",accuracy_score(y, proba_y_0_lt, normalize=True, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tpr and fpr based on different thresholds\n",
    "fpr, tpr, thresholds = metrics.roc_curve(yact,proba_y_0,pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501209003078432"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1685f4e7240>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=0, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model using training dataset\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
    "lr.fit(X_transform, yact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.64560104])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the intercept of the trained model (Theta_0)\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.910376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.498802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.343474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>-0.065449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.087607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_male</th>\n",
       "      <td>-1.285060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_Q</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_S</th>\n",
       "      <td>-0.179448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Coefficient\n",
       "Pclass        -0.910376\n",
       "Age           -0.498802\n",
       "SibSp         -0.343474\n",
       "Parch         -0.065449\n",
       "Fare           0.087607\n",
       "Sex_male      -1.285060\n",
       "Embarked_Q     0.000000\n",
       "Embarked_S    -0.179448"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the coefficients of independent attributes\n",
    "# the reason that we use the function .flatten() here is to convert the 8X1 array to 1X8 array\n",
    "coeff_df = pd.DataFrame(lr.coef_.flatten(), X.columns, columns=['Coefficient'])  \n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression(penalty='none', C=1.0, random_state=0, solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='none',\n",
       "                   random_state=0, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1.fit(X_transform, yact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(lr1.coef_.flatten(), X.columns, columns=['Coefficient'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.920327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.516214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.359431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>-0.075615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.095732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_male</th>\n",
       "      <td>-1.301682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_Q</th>\n",
       "      <td>-0.011757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_S</th>\n",
       "      <td>-0.190615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Coefficient\n",
       "Pclass        -0.920327\n",
       "Age           -0.516214\n",
       "SibSp         -0.359431\n",
       "Parch         -0.075615\n",
       "Fare           0.095732\n",
       "Sex_male      -1.301682\n",
       "Embarked_Q    -0.011757\n",
       "Embarked_S    -0.190615"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
